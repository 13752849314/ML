# Start At 2022/06/11

test

<div align=center><img src="../image/hg.png" alt="test"/></div>

$$
\frac{1}{e}

$$

## 梯度下降法

回归函数$h(x)=\theta_0x+\theta_1$

目标函数  $J(\theta_0,\theta_1)$

$$
J(\theta_0,\theta_1)=\frac{1}{2n}\sum_{i=1}^n(h(x_i)-y_i)^2

$$

$$
min \quad J(\theta_0,\theta_1)

$$

Start with some $\theta_0,\theta_1$

Keep changing $\theta_0,\theta_1$ to reduce $J(\theta_0,\theta_1)$ until we hopefully end up at minimum.

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \quad (j=0,1)

$$

$\alpha$称为学习率

$$
\begin{align*}
    \theta_0:&=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)\\
    &=\theta_0-\alpha\frac{\partial}{\partial\theta_0}(\frac{1}{2n}\sum_{i=1}^n(\theta_0x_i+\theta_1-y_i)^2)\\
    &=\theta_0-\alpha\frac{1}{n}\sum_{i=1}^nx_i(\theta_0x_i+\theta_1-y_i)\\
    &=\theta_0-\frac{\alpha}{n}\sum_{i=1}^nx_i(h(x_i)-y_i)
\end{align*}

$$

$$
\begin{align*}
    \theta_1:&=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)\\
    &=\theta_1-\alpha\frac{\partial}{\partial\theta_1}(\frac{1}{2n}\sum_{i=1}^n(\theta_0x_i+\theta_1-y_i)^2)\\
    &=\theta_1-\frac{\alpha}{n}\sum_{i=1}^n(h(x_i)-y_i)
\end{align*}

$$

$$
\begin{align*}
    \theta_j:&=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
    &=\theta_j-\alpha\frac{\partial}{\partial\theta_j}(\frac{1}{2n}\sum_{i=1}^n(h(x_i)-y_i)^2)\\
    &=\theta_j-\frac{\alpha}{n}\sum_{i=1}^n\frac{\partial{h}}{\partial\theta_j}(h(x_i)-y_i)
\end{align*}

$$

# End At 2022/6/27

# 多元线性回归 Start At 2022/7/1

$$
h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\dots+\theta_nx_n
$$
一般$x_0=1$

$$
J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2

$$

$$
\begin{align*}
    \theta_j:&=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1,\dots,\theta_n)\\
    &=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m\frac{\partial{h}}{\partial\theta_j}(h(x_i)-y_i)\\
    &=\theta_j-\frac{\alpha}{m}\sum_{i=1}^mx_i^j(h(x_i)-y_i)
\end{align*}
$$